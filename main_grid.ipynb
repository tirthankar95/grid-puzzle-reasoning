{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e311b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 13:22:10 [__init__.py:216] Automatically detected platform cpu.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from vllm import (\n",
    "    LLM, \n",
    "    SamplingParams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf97961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the farmers market, four friends each bought a different fruit at different prices. Each fruit had a different color wrapper. Can you determine which fruit each friend bought, its price, and wrapper color?\\nCategories:\\n['Price', 'Fruit', 'Color']\\n\\nPrice: ['$2.50', '$3.75', '$4.25', '$5.00']\\nFruit: ['Apple', 'Banana', 'Orange', 'Grape']\\nColor: ['Red', 'Yellow', 'Orange', 'Purple']\\n\\n\\nClues:\\n1. The fruit priced at $5.00 comes with a Purple wrapper. \\n2. The Banana costs $4.25 and comes with a Yellow wrapper. \\n3. The fruit priced at $2.50 is either the Grape or has the Orange wrapper. \\n4. The fruit priced at $3.75 is the Orange fruit.\\n\\n\\nWhile answering use the following format:\\nStep-by-step solution:\\nCreate a table out of your solution where each column represents the categories in the order listed above.</td>\n",
       "      <td>xyz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          question  \\\n",
       "0  At the farmers market, four friends each bought a different fruit at different prices. Each fruit had a different color wrapper. Can you determine which fruit each friend bought, its price, and wrapper color?\\nCategories:\\n['Price', 'Fruit', 'Color']\\n\\nPrice: ['$2.50', '$3.75', '$4.25', '$5.00']\\nFruit: ['Apple', 'Banana', 'Orange', 'Grape']\\nColor: ['Red', 'Yellow', 'Orange', 'Purple']\\n\\n\\nClues:\\n1. The fruit priced at $5.00 comes with a Purple wrapper. \\n2. The Banana costs $4.25 and comes with a Yellow wrapper. \\n3. The fruit priced at $2.50 is either the Grape or has the Orange wrapper. \\n4. The fruit priced at $3.75 is the Orange fruit.\\n\\n\\nWhile answering use the following format:\\nStep-by-step solution:\\nCreate a table out of your solution where each column represents the categories in the order listed above.   \n",
       "\n",
       "  answer  \n",
       "0    xyz  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Grid_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a119b2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tmittra/models/Qwen2-1.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "model_path = config[\"local_model_dir\"]\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09c4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_vllm(prompt, model_dir, temperature=0.7):\n",
    "    if LLM is None:\n",
    "        raise RuntimeError(\"vllm is not installed or failed to import.\")\n",
    "\n",
    "    llm = LLM(model=model_dir, \\\n",
    "            max_model_len=8192,\n",
    "            max_num_batched_tokens=8192,\n",
    "            dtype=\"float16\"\n",
    "        )\n",
    "    sampling_params = SamplingParams(temperature=temperature)\n",
    "\n",
    "    outputs = llm.generate(prompt, sampling_params=sampling_params)\n",
    "    for out in outputs:\n",
    "        # out.text is incremental; join full_text if needed. Keep simple:\n",
    "        print(\"=== vllm generation ===\")\n",
    "        print(out.outputs[0].text)\n",
    "        return out.outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69c9fa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 13:28:16 [utils.py:233] non-default args: {'dtype': 'float16', 'max_model_len': 8192, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'model': '/Users/tmittra/models/Qwen2-1.5B-Instruct'}\n",
      "INFO 01-11 13:28:16 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n",
      "WARNING 01-11 13:28:16 [model.py:1733] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 01-11 13:28:16 [model.py:1510] Using max model len 8192\n",
      "WARNING 01-11 13:28:16 [cpu.py:117] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 01-11 13:28:16 [arg_utils.py:1166] Chunked prefill is not supported for ARM and POWER and S390X CPUs; disabling it for V1 backend.\n",
      "INFO 01-11 13:28:22 [__init__.py:216] Automatically detected platform cpu.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:23 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:23 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/Users/tmittra/models/Qwen2-1.5B-Instruct', speculative_config=None, tokenizer='/Users/tmittra/models/Qwen2-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/Users/tmittra/models/Qwen2-1.5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=False, pooler_config=None, compilation_config={\"level\":2,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false,\"dce\":true,\"size_asserts\":false,\"nan_asserts\":false,\"epilogue_fusion\":true},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:23 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m WARNING 01-11 13:28:24 [cpu.py:316] Pin memory is not supported on CPU.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:24 [cpu_worker.py:66] Warning: NUMA is not enabled in this build. `init_cpu_threads_env` has no effect to setup thread affinity.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:24 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:24 [cpu_model_runner.py:106] Starting to load model /Users/tmittra/models/Qwen2-1.5B-Instruct...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:25 [cpu.py:104] Using Torch SDPA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W111 13:28:24.857088000 ProcessGroupGloo.cpp:545] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.95s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.95s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:30 [default_loader.py:267] Loading weights took 4.96 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:30 [kv_cache_utils.py:1087] GPU KV cache size: 149,792 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:30 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 18.29x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:30 [cpu_model_runner.py:117] Warming up model for the compilation...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m WARNING 01-11 13:28:30 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:38 [cpu_model_runner.py:121] Warming up done.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m INFO 01-11 13:28:38 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.40 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12734)\u001b[0;0m WARNING 01-11 13:28:38 [cpu.py:117] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 01-11 13:28:38 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993beaa8af434c019409e79579ad8bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcf186861a04a569934a3568ef628bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== vllm generation ===\n",
      "!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "df['response'] = df['question'].apply(lambda x: generate_with_vllm(x, model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac185029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
